{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "899a62af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.fina_rag import ComplaintRAG, load_prebuilt_vector_store\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fec0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG System...\n",
      " Loading RAG system...\n",
      " Loaded embedding model: all-MiniLM-L6-v2\n",
      "Loaded FAISS index with 41865 chunks\n",
      "ðŸ¤– Loading language model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "Device set to use cpu\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'ApertusForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'BltForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FlexOlmoForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MinistralForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'Qwen3NextForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SeedOssForCausalLM', 'SmolLM3ForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'VaultGemmaForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Language model ready\n",
      "RAG system ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing RAG System...\")\n",
    "rag = ComplaintRAG(vector_store_path=\"../vector_stores\")\n",
    "\n",
    "print(\"RAG system ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7be001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What are common issues with credit cards?\n",
      " Retrieving relevant complaints...\n",
      "Retrieved 5 relevant complaint chunks\n",
      " Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What are common issues with credit cards?\n",
      "ANSWER: \n",
      "Retrieved 5 relevant chunks\n",
      "\n",
      "Top sources used:\n",
      "\n",
      "Source 1:\n",
      "  Text: when bought the problems began s had a easy to work with card since the opened in mt i think i had o...\n",
      "  Product: Unknown\n",
      "  Issue: Unknown\n",
      "\n",
      "Source 2:\n",
      "  Text: is no longer avaialbe i have never had so many issues with a credit card in my life they are a bill ...\n",
      "  Product: Unknown\n",
      "  Issue: Unknown\n",
      "\n",
      "Source 3:\n",
      "  Text: issues with auto bills and create embarrassment when shopping at stores because the card declines ev...\n",
      "  Product: Unknown\n",
      "  Issue: Unknown\n"
     ]
    }
   ],
   "source": [
    "test_question = \"What are common issues with credit cards?\"\n",
    "answer, sources = rag.ask_question(test_question)\n",
    "print(f\"QUESTION: {test_question}\")\n",
    "print(f\"ANSWER: {answer}\")\n",
    "print(f\"Retrieved {len(sources)} relevant chunks\")\n",
    "\n",
    "# Show top sources\n",
    "print(\"\\nTop sources used:\")\n",
    "for i, source in enumerate(sources[:3]):\n",
    "    print(f\"\\nSource {i+1}:\")\n",
    "    print(f\"  Text: {source['text'][:100]}...\")\n",
    "    print(f\"  Product: {source['metadata'].get('product_category', 'Unknown')}\")\n",
    "    print(f\"  Issue: {source['metadata'].get('issue', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b691a3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Questions for Evaluation:\n",
      "1. Why are people unhappy with Credit Cards?\n",
      "2. What complaints exist about Personal Loans?\n",
      "3. Are there issues with money transfers?\n",
      "4. What problems do customers report with savings accounts?\n",
      "5. What is the most common complaint type?\n",
      "6. Are there any complaints about fraudulent charges?\n",
      "7. Do customers complain about customer service?\n",
      "8. What billing issues are reported?\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\n",
    "    (\"Why are people unhappy with Credit Cards?\", \"\"),\n",
    "    (\"What complaints exist about Personal Loans?\", \"\"),\n",
    "    (\"Are there issues with money transfers?\", \"\"),\n",
    "    (\"What problems do customers report with savings accounts?\", \"\"),\n",
    "    (\"What is the most common complaint type?\", \"\"),\n",
    "    (\"Are there any complaints about fraudulent charges?\", \"\"),\n",
    "    (\"Do customers complain about customer service?\", \"\"),\n",
    "    (\"What billing issues are reported?\", \"\"),\n",
    "]\n",
    "\n",
    "print(\"Test Questions for Evaluation:\")\n",
    "for i, (q, _) in enumerate(test_questions, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f128671b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running Evaluation...\n",
      "Question 1: Why are people unhappy with Credit Cards?\n",
      "\n",
      "Question: Why are people unhappy with Credit Cards?\n",
      " Retrieving relevant complaints...\n",
      "Retrieved 5 relevant complaint chunks\n",
      " Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ...\n",
      "Sources retrieved: 5\n",
      "Score recorded: 1/5\n",
      "Question 2: What complaints exist about Personal Loans?\n",
      "\n",
      "Question: What complaints exist about Personal Loans?\n",
      " Retrieving relevant complaints...\n",
      "Retrieved 5 relevant complaint chunks\n",
      " Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ...\n",
      "Sources retrieved: 5\n",
      "Score recorded: 1/5\n",
      "Question 3: Are there issues with money transfers?\n",
      "\n",
      "Question: Are there issues with money transfers?\n",
      " Retrieving relevant complaints...\n",
      "Retrieved 5 relevant complaint chunks\n",
      " Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ...\n",
      "Sources retrieved: 5\n",
      "Score recorded: 1/5\n",
      "Question 4: What problems do customers report with savings accounts?\n",
      "\n",
      "Question: What problems do customers report with savings accounts?\n",
      " Retrieving relevant complaints...\n",
      "Retrieved 5 relevant complaint chunks\n",
      " Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ...\n",
      "Sources retrieved: 5\n",
      "Score recorded: 122/5\n",
      "Question 5: What is the most common complaint type?\n",
      "\n",
      "Question: What is the most common complaint type?\n",
      " Retrieving relevant complaints...\n",
      "Retrieved 5 relevant complaint chunks\n",
      " Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ...\n",
      "Sources retrieved: 5\n",
      "Score recorded: /5\n",
      "Question 6: Are there any complaints about fraudulent charges?\n",
      "\n",
      "Question: Are there any complaints about fraudulent charges?\n",
      " Retrieving relevant complaints...\n",
      "Retrieved 5 relevant complaint chunks\n",
      " Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ...\n",
      "Sources retrieved: 5\n",
      "Score recorded: /5\n",
      "Question 7: Do customers complain about customer service?\n",
      "\n",
      "Question: Do customers complain about customer service?\n",
      " Retrieving relevant complaints...\n",
      "Retrieved 5 relevant complaint chunks\n",
      " Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ...\n",
      "Sources retrieved: 5\n",
      "Score recorded: /5\n",
      "Question 8: What billing issues are reported?\n",
      "\n",
      "Question: What billing issues are reported?\n",
      " Retrieving relevant complaints...\n",
      "Retrieved 5 relevant complaint chunks\n",
      " Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ...\n",
      "Sources retrieved: 5\n",
      "Score recorded: /5\n"
     ]
    }
   ],
   "source": [
    "print(\" Running Evaluation...\")\n",
    "eval_results = []\n",
    "\n",
    "for i, (question, expected) in enumerate(test_questions):\n",
    "    print(f\"Question {i+1}: {question}\")\n",
    "    answer, sources = rag.ask_question(question, top_k=5)\n",
    "    print(f\"Answer: {answer[:150]}...\")\n",
    "    print(f\"Sources retrieved: {len(sources)}\")\n",
    "    score = input(\"Enter quality score (1-5): \")\n",
    "    source_preview = []\n",
    "    for src in sources[:2]:\n",
    "        product = src['metadata'].get('product_category', 'Unknown')\n",
    "        issue = src['metadata'].get('issue', 'Unknown')\n",
    "        source_preview.append(f\"{product}: {issue}\")\n",
    "    \n",
    "    eval_results.append({\n",
    "        'Question': question,\n",
    "        'Generated Answer': answer[:200] + \"...\" if len(answer) > 200 else answer,\n",
    "        'Retrieved Sources': \", \".join(source_preview),\n",
    "        'Quality Score': int(score) if score.isdigit() else 3,\n",
    "        'Comments': 'Add your analysis here'\n",
    "    })\n",
    "    \n",
    "    print(f\"Score recorded: {score}/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eval_df = pd.DataFrame(eval_results)\n",
    "\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(eval_df.to_string(index=False))\n",
    "eval_df.to_csv('../evaluation_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'evaluation_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    question = input(\"Ask a question (or 'quit' to exit): \")\n",
    "    \n",
    "    if question.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    answer, sources = rag.ask_question(question)\n",
    "    \n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    print(f\"\\nSources found: {len(sources)}\")\n",
    "    \n",
    "    if sources:\n",
    "        print(\"\\nTop source preview:\")\n",
    "        print(f\"Product: {sources[0]['metadata'].get('product_category', 'Unknown')}\")\n",
    "        print(f\"Issue: {sources[0]['metadata'].get('issue', 'Unknown')}\")\n",
    "        print(f\"Text: {sources[0]['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_table = \"| Question | Generated Answer | Retrieved Sources | Quality Score | Comments |\\n\"\n",
    "markdown_table += \"|----------|------------------|-------------------|---------------|----------|\\n\"\n",
    "\n",
    "for _, row in eval_df.iterrows():\n",
    "    answer_short = row['Generated Answer'][:80] + \"...\" if len(row['Generated Answer']) > 80 else row['Generated Answer']\n",
    "    sources_short = row['Retrieved Sources'][:50] + \"...\" if len(row['Retrieved Sources']) > 50 else row['Retrieved Sources']\n",
    "    \n",
    "    markdown_table += f\"| {row['Question'][:50]}... | {answer_short} | {sources_short} | {row['Quality Score']} | {row['Comments'][:30]}... |\\n\"\n",
    "\n",
    "print(\"Markdown Table for Report:\")\n",
    "print(\"\\n\" + markdown_table)\n",
    "with open('../evaluation_table.md', 'w') as f:\n",
    "    f.write(markdown_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
